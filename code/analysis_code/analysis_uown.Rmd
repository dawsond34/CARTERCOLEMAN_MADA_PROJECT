---
title: "analysis_uown"
author: "C. Coleman"
date: "10/27/2021"
output:
  word_document: default
  html_document: default
---

# UOWN Data Analysis for Determining Correlation Between Stream Health Indicators.
</br>
_The following markdown conducts the statistical analysis of how different indicators of stream health can potentially predict each other. In the previous processing script, the main take away was that there were very few significant relationships between different chemical, fecal/microbial and biological indicators. The only significant relationships found between Turbity and E. Coli CFU. However, It struck me as peculiar to see biological indicator score and conductivity to have only one significant relationship. After further review of the exploratory figures, I now want to assess the data and relationships explored thus far for impact of outliers, determine if it is appropriate to remove outlying values, and see if this helps find more significant relationships._
_The second part will include test/train splitting to predict stream health indicator values in the MIDO data set and assess the fit of linear models to the MIDO data. This is a more depth recreation of the linear model exploration. Obviously, we found that there weren't any significant relationships in exploratory analysis. However, this gives us a chance to assess how good a fit the models are in order to see if its even worth looking at the relationships._ 

</br>

_Therefore, the following constitutes the statistical analysis protocol that will be conducted in this markdown:_
</br>
#### PART 1: Outliers in stream health indicator (Biological Score, E. Coli (cfu), Conductivity, NO3 (mg/L), and Turbidity) linear relationships.
_1. Assess each linear relationship previously performed for outliers in the data;_
_2. Determine if it is appropriate to remove outliers;_
_3. Rerun linear regressions for the different stream health indicator relationships and assess if more significant relationships exist._

</br>

#### PART 2: Linear Model Fit Analysis of MIDO Data
_1. Conduct test/train data splitting for MIDO Data._
_2. Conduct linear modeling for relationships between stream health indicator predictors (Biological Score, E. Coli (cfu), Conductivity, NO3 (mg/L), and Turbidity) in the form of a workflow for the Test and Train MIDO data._
_3. Predict values for Test and Train MIDO data._
_4. Assess how well the MIDO linear model suite of relationships (Biological Score, E. Coli (cfu), Conductivity, NO3 (mg/L), and Turbidity) fir the test and train data using Root Mean Square Error (rmse). The result will measure how good of a fit the created models are to the data._

</br>

#### PART 3: LASSO Modeling MIDO Data
_Conduct LASSO modeling of linear regression between Biological Score and all other variables and E. coli CFU and all other variables. Cross validation well be done by calculating RMSE for all models and compared to the RMSE for linear regression modeling of a Null model. Additionally, The best model will be selected using the R function select_best(), for which residuals will be calculated between predicted and actual data._

### Required packages
```{r}
library(tidyverse) #Working with multiple Tidy packages
library(tidymodels) #Building models
library(dplyr) #data manipulation
library(here) #setting pathways for saving files
library(rpart) #Model fitting
library(ranger) #Model fitting
library(glmnet) #Model fitting
library(knitr) #saving tables
```


### Load Data
_Start by loading the cleaned data labeled as MIDO, NORO, and BICO into the Global Environment._
```{r}
#Set the location of the desired dataframes
MIDO_location <- here::here("data","processed_data","MIDO.RDS")
NORO_location <- here::here("data","processed_data","NORO.RDS")
BICO_location <- here::here("data","processed_data","BICO.RDS")

#Load in the Dataframes
MIDO <- readRDS(MIDO_location)
NORO <- readRDS(NORO_location)
BICO <- readRDS(BICO_location)
```
</br>
_Additionally, I am going to start of by log fitting the E. coli data. This is because that data can have counts several orders of magnitude higher than the variables it is being assessed with. Therefore, model fit may be artificially bad.
```{r}
#Change the E.Coli (cfu) variable to log10 scale using mutate()
MIDO %>%
  mutate_at(vars(e.coli.cfu), ~log10(.))
NORO %>%
  mutate_at(vars(e.coli.cfu), ~log10(.))
BICO %>%
  mutate_at(vars(e.coli.cfu), ~log10(.))
```

</br>

### Load all the created linear models from the processing data markdown.
_We need to load the exploratory linear models created previously in order to assess them for outliers that impact the linear regressions. This is because the cooksdistance() function calls for a linear model input.

</br>

#### MIDO

</br>

_Biological Score_
```{r}
#Loading exploratory linear models for MIDO data with Biological Score as Outcome variable.
BS_Con_location <- here::here("data","processed_data","BS_Con.rds")
bs_con_lm <- readRDS(BS_Con_location)

BS_ECFU_location <- here::here("data","processed_data","BS_ECFU.rds")
bs_ecfu_lm <- readRDS(BS_ECFU_location)

BS_no3_location <- here::here("data","processed_data","BS_no3.rds")
bs_no3_lm <- readRDS(BS_no3_location)

BS_Turb_location <- here::here("data","processed_data","BS_Turb.rds")
bs_turb_lm <- readRDS(BS_Turb_location)
```

</br>

_ECFU_
```{r}
#Loading exploratory linear models for MIDO data with E. coli (cfu) as Outcome variable.
ECFU_Con_location <- here::here("data","processed_data","BS_Con.rds")
ecfu_con_lm <- readRDS(ECFU_Con_location)

ECFU_no3_location <- here::here("data","processed_data","BS_no3.rds")
ecfu_no3_lm <- readRDS(ECFU_no3_location)

ECFU_Turb_location <- here::here("data","processed_data","BS_Turb.rds")
ecfu_turb_lm <- readRDS(ECFU_Turb_location)
```

</br>

#### BICO

</br>

_Biological Score_
```{r}
#Loading exploratory linear models for BICO data with Biological Score as Outcome variable.
B_BS_Con_location <- here::here("data","processed_data","B_BS_Con.rds")
b_bs_con_lm <- readRDS(B_BS_Con_location)

B_BS_ECFU_location <- here::here("data","processed_data","B_BS_ECFU.rds")
b_bs_ecfu_lm <- readRDS(B_BS_ECFU_location)

B_BS_no3_location <- here::here("data","processed_data","B_BS_no3.rds")
b_bs_no3_lm <- readRDS(B_BS_no3_location)

B_BS_Turb_location <- here::here("data","processed_data","B_BS_Turb.rds")
b_bs_turb_lm <- readRDS(B_BS_Turb_location)
```

</br>

_ECFU_
```{r}
#Loading exploratory linear models for BICO data with E. coli (cfu) as Outcome variable.
B_ECFU_Con_location <- here::here("data","processed_data","B_BS_Con.rds")
b_ecfu_con_lm <- readRDS(B_ECFU_Con_location)

B_ECFU_no3_location <- here::here("data","processed_data","B_BS_no3.rds")
b_ecfu_no3_lm <- readRDS(B_ECFU_no3_location)

B_ECFU_Turb_location <- here::here("data","processed_data","B_BS_Turb.rds")
b_ecfu_turb_lm <- readRDS(B_ECFU_Turb_location)
```

</br>

#### NORO

</br>

_Biological Score_
```{r}
#Loading exploratory linear models for NORO data with Biological Score as Outcome variable.
N_BS_Con_location <- here::here("data","processed_data","N_BS_Con.rds")
n_bs_con_lm <- readRDS(N_BS_Con_location)

N_BS_ECFU_location <- here::here("data","processed_data","N_BS_ECFU.rds")
n_bs_ecfu_lm <- readRDS(N_BS_ECFU_location)

N_BS_no3_location <- here::here("data","processed_data","N_BS_no3.rds")
n_bs_no3_lm <- readRDS(N_BS_no3_location)

N_BS_Turb_location <- here::here("data","processed_data","N_BS_Turb.rds")
n_bs_turb_lm <- readRDS(N_BS_Turb_location)
```

</br>

_ECFU_
```{r}
#Loading exploratory linear models for NORO data with E. coli (cfu) as Outcome variable.
N_ECFU_Con_location <- here::here("data","processed_data","N_BS_Con.rds")
n_ecfu_con_lm <- readRDS(N_ECFU_Con_location)

N_ECFU_no3_location <- here::here("data","processed_data","N_BS_no3.rds")
n_ecfu_no3_lm <- readRDS(N_ECFU_no3_location)

N_ECFU_Turb_location <- here::here("data","processed_data","N_BS_Turb.rds")
n_ecfu_turb_lm <- readRDS(N_ECFU_Turb_location)
```

</br>

### Outlier identification using Cook's Distance.

</br>

_Outliers in these data could be potentially ttreated as errors in reporting. This is because the data is predominantly collected by non-scientist volunteers, who have had training with scientists. As such, it is likely that sampling of mis-identification of macroinvertebrates taxa may occurred. Therefore, an argument can be made that assessing for anomalies in the data could help tease apart the true relationships that exist between different stream health indicators._

</br>

_As such, we will start by analyzing the data for outliers by evaluating relationships with cook's distance._

</br>

#### Set sample size for sample size parameter (4/n) for each data set (MIDO, NORO, BICO)
```{r}
#This makes an argument that makes the sum number of rows in a particular data set equal to the sample size of the data set and names it accordingly.
sample_size_MIDO <- nrow(MIDO)
sample_size_NORO <- nrow(NORO)
sample_size_BICO <- nrow(BICO)
```

</br>

#### MIDO

</br>

_Outcome: Biological Score_
```{r}
#Creates a dataframe that stores the cook's distance analysis of influential data points.
bs_con_cd <- cooks.distance(bs_con_lm)
plot(bs_con_cd, pch="*", cex=2, main="MIDO: Conductivity vs. Biological Score with Outliers")
abline(h = 4/sample_size_MIDO, col="red")

bs_ecfu_cd <- cooks.distance(bs_ecfu_lm)
plot(bs_ecfu_cd, pch="*", cex=2, main="MIDO: E. coli CFU vs. Biological Score with Outliers")
abline(h = 4/sample_size_MIDO, col="red")

bs_no3_cd <- cooks.distance(bs_no3_lm)
plot(bs_no3_cd, pch="*", cex=2, main="MIDO: NO3 vs. Biological Score with Outliers")
abline(h = 4/sample_size_MIDO, col="red")

bs_turb_cd <- cooks.distance(bs_turb_lm)
plot(bs_turb_cd, pch="*", cex=2, main="MIDO: Turbidity vs. Biological Score with Outliers")
abline(h = 4/sample_size_MIDO, col="red")
```
</br>
_Outcome: E.coli CFU_
```{r}
#Creates a dataframe that stores the cook's distance analysis of influential data points.
ecfu_con_cd <- cooks.distance(ecfu_con_lm)
plot(ecfu_con_cd, pch="*", cex=2, main="MIDO: Conductivity vs. E. coli CFU with Outliers")
abline(h = 4/sample_size_MIDO, col="red")

ecfu_no3_cd <- cooks.distance(ecfu_no3_lm)
plot(ecfu_no3_cd, pch="*", cex=2, main="MIDO: NO3 vs. E. coli CFU with Outliers")
abline(h = 4/sample_size_MIDO, col="red")

ecfu_turb_cd <- cooks.distance(ecfu_turb_lm)
plot(ecfu_turb_cd, pch="*", cex=2, main="MIDO: Turbidity vs. E. coli CFU with Outliers")
abline(h = 4/sample_size_MIDO, col="red")
```
</br>

#### NORO

</br>
_Outcome: Biological Score_
```{r}
#Creates a dataframe that stores the cook's distance analysis of influential data points.
n_bs_con_cd <- cooks.distance(n_bs_con_lm)
plot(n_bs_con_cd, pch="*", cex=2, main="NORO: Conductivity vs. Biological Score with Outliers")
abline(h = 4/sample_size_NORO, col="red")

n_bs_ecfu_cd <- cooks.distance(n_bs_ecfu_lm)
plot(n_bs_ecfu_cd, pch="*", cex=2, main="NORO: E. coli CFU vs. Biological Score with Outliers")
abline(h = 4/sample_size_NORO, col="red")

n_bs_no3_cd <- cooks.distance(n_bs_no3_lm)
plot(n_bs_no3_cd, pch="*", cex=2, main="NORO: NO3 vs. Biological Score with Outliers")
abline(h = 4/sample_size_NORO, col="red")

n_bs_turb_cd <- cooks.distance(n_bs_turb_lm)
plot(n_bs_turb_cd, pch="*", cex=2, main="NORO: Turbidity vs. Biological Score with Outliers")
abline(h = 4/sample_size_NORO, col="red")
```
</br>

_Outcome: E.coli CFU_
```{r}
#Creates a dataframe that stores the cook's distance analysis of influential data points.
n_ecfu_con_cd <- cooks.distance(n_ecfu_con_lm)
plot(n_ecfu_con_cd, pch="*", cex=2, main="NORO: Conductivity vs. E. coli CFU with Outliers")
abline(h = 4/sample_size_NORO, col="red")

n_ecfu_no3_cd <- cooks.distance(n_ecfu_no3_lm)
plot(n_ecfu_no3_cd, pch="*", cex=2, main="NORO: NO3 vs. E. coli CFU with Outliers")
abline(h = 4/sample_size_NORO, col="red")

n_ecfu_turb_cd <- cooks.distance(n_ecfu_turb_lm)
plot(n_ecfu_turb_cd, pch="*", cex=2, main="NORO: Turbidity vs. E. coli CFU with Outliers")
abline(h = 4/sample_size_NORO, col="red")
```
</br>

#### BICO

</br>

_Outcome: Biological Score_
```{r}
#Creates a dataframe that stores the cook's distance analysis of influential data points.
b_bs_con_cd <- cooks.distance(b_bs_con_lm)
plot(b_bs_con_cd, pch="*", cex=2, main="BICO: Conductivity vs. Biological Score with Outliers")
abline(h = 4/sample_size_BICO, col="red")

b_bs_ecfu_cd <- cooks.distance(b_bs_ecfu_lm)
plot(b_bs_ecfu_cd, pch="*", cex=2, main="BICO: E. Coli CFU vs. Biological Score with Outliers")
abline(h = 4/sample_size_BICO, col="red")

b_bs_no3_cd <- cooks.distance(b_bs_no3_lm)
plot(b_bs_no3_cd, pch="*", cex=2, main="BICO: NO3 vs. Biological Score with Outliers")
abline(h = 4/sample_size_BICO, col="red")

b_bs_turb_cd <- cooks.distance(bs_turb_lm)
plot(b_bs_turb_cd, pch="*", cex=2, main="BICO: Turbidity vs. Biological Score with Outliers")
abline(h = 4/sample_size_BICO, col="red")
```
</br>

_Outcome: E.coli CFU_
```{r}
#Creates a dataframe that stores the cook's distance analysis of influential data points.
b_ecfu_con_cd <- cooks.distance(b_ecfu_con_lm)
plot(b_ecfu_con_cd, pch="*", cex=2, main="BICO: Conductivity vs. E. coli CFU with Outliers")
abline(h = 4/sample_size_BICO, col="red")

b_ecfu_no3_cd <- cooks.distance(b_ecfu_no3_lm)
plot(b_ecfu_no3_cd, pch="*", cex=2, main="BICO: NO3 vs. E. coli CFU with Outliers")
abline(h = 4/sample_size_BICO, col="red")

b_ecfu_turb_cd <- cooks.distance(b_ecfu_turb_lm)
plot(b_ecfu_turb_cd, pch="*", cex=2, main="BICO: Turbidity vs. E. coli CFU with Outliers")
abline(h = 4/sample_size_BICO, col="red")
```
</br>

_The results of the Cook's Distance show that while there are outliers, very few of them actually exceed a Cook's Distance value (CDV) of 1. The CDV value of 1 is significant because it is the statistical community consensus value that indicates an influential outlier. As such, it is my educated opinion to leave all data points in, as I do not believe they are skewing liner modle results in a significant enough way to warrant removal._

</br>

# Part 2: Linear Model Fit Assessment for MIDO Data

</br>

_At this point,the main take-aways from data analysis thus far is that correlation of stream health indicators showed that most interactions were non-significant, with the exception of E. coli (cfu) and turbidity. Additionally, the data set is most likely not skewed by influential outliers._

</br>

_Moving forward, I want to focus in on just the MIDO dataset. As you can tell, working with many variables in three different data sets can be quite cumbersome to read through. For ease of interpretation, the MIDO data set will serve as the data frame where I test how robust the linear fit models actually are for the data. MIDO is the only data set large enough that when we split the data set into a "test" and "train" data set, I actually have confidence in the resulting model fit analysis. For comparison, MIDO has an n = 88; NORO has an n = 35; and BICO has an n = 23. The only thing we are missing by dropping the other sites is assessing site-by-site comparison, which I am choosing not to focus on anyway._

</br>

#### Splitting MIDO Data
_As discussed above, we need to split the MIDO data into a test and train data set. We do this to see if we get approximately the same linear relationship between the larger (75% of MIDO) train data and the smaller (25% of MIDO) test data._
```{r}
#Split the data with 1:4 ratio.
data_split <- initial_split(MIDO, prop = 3/4)

# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
```

</br>

_Create a recipe for each linear model relationship in the MIDO dataframe._
```{r}
Recipe_bs_con <- recipe(biological_score ~ conductivity.uscm, data = MIDO)
Recipe_bs_ecfu <- recipe(biological_score ~ e.coli.cfu, data = MIDO)
Recipe_bs_no3 <- recipe(biological_score ~ no3.mgL, data = MIDO)
Recipe_bs_turb <- recipe(biological_score ~ turbidity.ntu, data = MIDO)
Recipe_ecfu_con <- recipe(e.coli.cfu ~ e.coli.cfu, data = MIDO)
Recipe_ecfu_no3 <- recipe(e.coli.cfu ~ no3.mgL, data = MIDO)
Recipe_ecfu_turb <- recipe(e.coli.cfu ~ turbidity.ntu, data = MIDO)
```

</br>

_With our seven linear model recipes created, we will define of linear regression model pipe._
```{r}
linear_mod <- linear_reg() %>% 
  set_engine("lm") %>%
  set_mode("regression")
```

</br>

_Next, create a workflow that adds our seven recipes and model together._
```{r}
#Biological Score and Conductivity
bs_con_wflow <- 
  workflow() %>% 
  add_model(linear_mod) %>% 
  add_recipe(Recipe_bs_con)

#Biological Score and E. coli(cfu)
bs_ecfu_wflow <- 
  workflow() %>% 
  add_model(linear_mod) %>% 
  add_recipe(Recipe_bs_ecfu)

#Biological Score and NO3
bs_no3_wflow <- 
  workflow() %>% 
  add_model(linear_mod) %>% 
  add_recipe(Recipe_bs_no3)

#Biological Score and Turbidity
bs_turb_wflow <- 
  workflow() %>% 
  add_model(linear_mod) %>% 
  add_recipe(Recipe_bs_turb)

#E. Coli (cfu) and Conductivity
ecfu_con_wflow <- 
  workflow() %>% 
  add_model(linear_mod) %>% 
  add_recipe(Recipe_ecfu_con)

#E. Coli (cfu) and NO3
ECFU_no3_wflow <- 
  workflow() %>% 
  add_model(linear_mod) %>% 
  add_recipe(Recipe_ecfu_no3)

#E. Coli (cfu) and Turbidity
ecfu_turb_wflow <- 
  workflow() %>% 
  add_model(linear_mod) %>% 
  add_recipe(Recipe_ecfu_turb)
```  

</br>

## Modeling Train Data Predictions Using Workflows
_Using the workflow above, lets now fit each pf the seven stream health indicator linear models to the train data set._

</br>

_Defining a command that runs model fitting to each of the seven models_
```{r}
bs_con_fit <- 
  bs_con_wflow %>% 
  fit(data = train_data)

bs_ecfu_fit <- 
  bs_ecfu_wflow %>% 
  fit(data = train_data)

bs_no3_fit <- 
  bs_no3_wflow %>% 
  fit(data = train_data)

bs_turb_fit <- 
  bs_turb_wflow %>% 
  fit(data = train_data)

ecfu_con_fit <- 
  ecfu_con_wflow %>% 
  fit(data = train_data)

ecfu_no3_fit <- 
  ECFU_no3_wflow %>% 
  fit(data = train_data)

ecfu_turb_fit <- 
  ecfu_turb_wflow %>% 
  fit(data = train_data)
```

</br>

_Pull linear regression fit models using parsnip()._
```{r}
bs_con_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

bs_ecfu_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

bs_no3_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

bs_turb_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

Ecoli_con_lm <- ecfu_con_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

ecfu_no3_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()

ecfu_turb_fit %>% 
  extract_fit_parsnip() %>% 
  tidy()
```
</br>

#### Using Workflows to Make Predictions in the train data set.
_This is important because we are basically going to try and recreate the actual MIDO data set using the test and trained data and our linear models. We will eventually compare the test data predictions with the train data predictions to assess if our model is actually a good estimator of the relationships between stream health indicators. Additionally, this prediction step allows us to "artificially" create a larger n for our data, allowing fit to be assessed more accurately. This will be done using the augment() function to make predicted outcomes in the test and train data._
```{r}
bs_con_aug <- 
  augment(bs_con_fit, train_data)

bs_ecfu_aug <- 
  augment(bs_ecfu_fit, train_data)

bs_no3_aug <- 
  augment(bs_no3_fit, train_data)

bs_turb_aug <- 
  augment(bs_turb_fit, train_data)

ecfu_con_aug <- 
  augment(ecfu_con_fit, train_data)

ecfu_no3_aug <- 
  augment(ecfu_no3_fit, train_data)

ecfu_turb_aug <- 
  augment(ecfu_turb_fit, train_data)
```

</br>

#### Using Root Mean Squared Error (RMSE) to Assess Model Fit.

</br>

_Assessing fit for train data predictions by calculating RMSE for the test data predictions of stream health indicator linear relationships._
```{r}
bs_con_aug %>%
  rmse(truth = biological_score, .pred)

bs_ecfu_aug %>%
  rmse(truth = biological_score, .pred)

bs_no3_aug %>%
  rmse(truth = biological_score, .pred)

bs_turb_aug %>%
  rmse(truth = biological_score, .pred)
 
Ecoli_con_RMSE <- ecfu_con_aug %>%
  rmse(truth = e.coli.cfu, .pred)
  
ecfu_no3_aug %>%
  rmse(truth = e.coli.cfu, .pred)
  
ecfu_turb_aug %>%
  rmse(truth = e.coli.cfu, .pred)
```

</br>

## Modeling Test Data Predictions Using Workflows
_Using the workflow above, lets now fit each pf the seven stream health indicator linear models to the test data set._

</br>

_Defining a command that runs model fitting to each of the seven models_
```{r}
bs_con_fit_test <- 
  bs_con_wflow %>% 
  fit(data = test_data)

bs_ecfu_fit_test <- 
  bs_ecfu_wflow %>% 
  fit(data = test_data)

bs_no3_fit_test <- 
  bs_no3_wflow %>% 
  fit(data = test_data)

bs_turb_fit_test <- 
  bs_turb_wflow %>% 
  fit(data = test_data)

ecfu_con_fit_test <- 
  ecfu_con_wflow %>% 
  fit(data = test_data)

ecfu_no3_fit_test <- 
  ECFU_no3_wflow %>% 
  fit(data = test_data)

ecfu_turb_fit_test <- 
  ecfu_turb_wflow %>% 
  fit(data = test_data)
```

</br>

_Pull linear regression fit models using parsnip()._
```{r}
bs_con_fit_test %>% 
  extract_fit_parsnip() %>% 
  tidy()

bs_ecfu_fit_test %>% 
  extract_fit_parsnip() %>% 
  tidy()

bs_no3_fit_test %>% 
  extract_fit_parsnip() %>% 
  tidy()

bs_turb_fit_test %>% 
  extract_fit_parsnip() %>% 
  tidy()

Ecoli_con_test_lm <- ecfu_con_fit_test %>% 
  extract_fit_parsnip() %>% 
  tidy()

ecfu_no3_fit_test %>% 
  extract_fit_parsnip() %>% 
  tidy()

ecfu_turb_fit_test %>% 
  extract_fit_parsnip() %>% 
  tidy()
```

</br>

#### Using Workflows to Make Predictions in the test data set.
_This is important because we are basically going to try and recreate the actual MIDO data set using the test and trained data and our linear models. This prediction step allows us to "artificially" create a larger n for our data, allowing fit to be assessed more accurately for data predicted from split data. This will be done using the augment() function to make predicted outcomes in the test and train data._
```{r}
bs_con_aug_test <- 
  augment(bs_con_fit, test_data)

bs_ecfu_aug_test <- 
  augment(bs_ecfu_fit, test_data)

bs_no3_aug_test <- 
  augment(bs_no3_fit, test_data)

bs_turb_aug_test <- 
  augment(bs_turb_fit, test_data)

ecfu_con_aug_test <- 
  augment(ecfu_con_fit, test_data)

ecfu_no3_aug_test <- 
  augment(ecfu_no3_fit, test_data)

ecfu_turb_aug_test <- 
  augment(ecfu_turb_fit, test_data)
```  

</br>

#### Using Root Mean Squared Error (RMSE) to Assess Model Fit.

_Assessing fit for train data predictions by calculating RMSE for the test data predictions of stream health indicator linear relationships._
```{r}
bs_con_aug_test %>%
  rmse(truth = biological_score, .pred)

bs_ecfu_aug_test %>%
  rmse(truth = biological_score, .pred)

bs_no3_aug_test %>%
  rmse(truth = biological_score, .pred)

bs_turb_aug_test %>%
  rmse(truth = biological_score, .pred)
 
Ecoli_con_RMSE_test <- ecfu_con_aug_test %>%
  rmse(truth = e.coli.cfu, .pred)
  
ecfu_no3_aug_test %>%
  rmse(truth = e.coli.cfu, .pred)
  
ecfu_turb_aug_test %>%
  rmse(truth = e.coli.cfu, .pred)
```
</br>

#### Save significant Test and Train linear models to the results folder.
```{r}
#Create a table for E. coli (cfu) as a function of Conductivity Linear Fit Model in the MIDO train data.
MIDO_ECFU_Con_fit <- kable(as.tibble(Ecoli_con_lm),
      caption = "Linear Fit Model: E. Coli CFU predicted by Conductivity in the Trained MIDO Data Set")

MIDO_ECFU_Con_fit

#Create a table for E. coli (cfu) as a function of Conductivity Linear Fit Model in the MIDO test data.

MIDO_ECFU_Con_fit_test <- kable(as.tibble(Ecoli_con_test_lm),
      caption = "Linear Fit Model: E. Coli CFU predicted by Conductivity in the Test MIDO Data Set")

MIDO_ECFU_Con_fit_test
```

</br>

#### Save significant Test and Train RMSE to the results folder.
```{r}
#Create a table for E. coli (cfu) as a function of Conductivity Linear Fit Model RMSE in the MIDO train data.
MIDO_ECFU_Con_RMSE <- kable(as.tibble(Ecoli_con_RMSE),
      caption = "Root Mean Square Error: E. Coli CFU predicted by Conductivity in the Trained MIDO Data Set")

MIDO_ECFU_Con_RMSE

#Create a table for E. coli (cfu) as a function of Conductivity Linear Fit Model RMSE in the MIDO test data.

MIDO_ECFU_Con_RMSE_test <- kable(as.tibble(Ecoli_con_RMSE),
      caption = "Root Mean Square Error: E. Coli CFU predicted by Conductivity in the Test MIDO Data Set")

MIDO_ECFU_Con_RMSE_test
```

# Part 3: LASSO Modeling for Ideal Combination of Stream Health Indicators to Predict Biological SCore and E. Coli Colony Forming Units in MIDO Data.

# Biological Score

## Data Set Up:

_Take ID and time variables out of MIDO Data frame. This will help reduce errors when we try to run the Model._
```{r}
MIDO_LASSO <- MIDO %>%
  select(-"WSID", -"stream_ID", -"month", -"year", -"day", -"pH")
```

</br>

_log transform E. coli data to fix bias in E. coli data_
```{r}
MIDO_LASSO <- MIDO_LASSO %>%
  mutate(e.coli.cfu = log10(MIDO_LASSO$e.coli.cfu))
```

</br>

_Set seed: This sets a random number generator with initial (pseudo)random values set as "123". We will need a series of random numbers created for our machine learning analysis._
```{r}
#Set random number generator.
set.seed(123)
```


</br>

## Null Model For MIDO Data

_5-fold cross validation, 5 times repeated for MIDO data: Here we are setting a cross-validation of the machine learning models. Cross-validation is used to measure how the results of our machine learning models will generalize to an independent data set. As such, the folds created will be be 5 random sub-samples of the train data set to test the validity of our models within the train data set. The 5x5 structure is arbitrary._
```{r}
#Create folds for LASSO Cross Validation.
fold_BS <- vfold_cv(MIDO_LASSO, v = 5, repeats = 5, strata = biological_score)
```

_Creating the recipe for Biological Score vs all predictors_

```{r}
BS.recipe <- 
  recipe(biological_score ~ ., data = MIDO_LASSO) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())
  
BS.recipe
```
_Setting linear regression model to assess relationship between Biological Score (outcome) and all other predictor variables._
```{r}
lm_mod <- linear_reg() %>% set_engine("lm") %>% set_mode("regression")
```

_However, first we need to create our null model to test against._

</br>

### Null Model:

 </br>

_Creates null model recipe. When we call this term, it will indicate in our workflow that Biological Score will be predicted by a value of 1 (NULL)._ 
```{r}
Null_recipe_lm <- recipe(biological_score ~ 1, data = MIDO_LASSO)
```

_Creating the Workflow: this creates a set workflow for running a null linear regression model with biological score as the outcome._
```{r}
null_wf <- workflow() %>% add_model(lm_mod) %>% add_recipe(Null_recipe_lm)
```

_Here, I am going to fit the null model created in the above workflow to the folds made from the train data set._
```{r}
null_lm <- fit_resamples(null_wf, resamples = fold_BS)
```
_Calculate RMSE for the Null linear model._
```{r}
Null_Met <- collect_metrics(null_lm)

Null_Met <- kable(Null_Met,
      caption = "Root Mean Squared Error and Standard Deviation for Biological Score Null Linear Model")

Null_Met
```
_RMSE = 6.63, with a standard deviation of 0.13. This will serve as our check to test our models against latter on._

</br>

## LASSO Model

</br>

_Specifying The Model: LASSO_
```{r}
lasso_mod <- 
  linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

</br>

_Creating a Workflow: LASSO_
```{r}
lasso_wf <- workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(BS.recipe)
```

</br>

_Create Tuning Grid: LASSO_
```{r}
LASSO_grid <- grid_regular(penalty(), levels = 50)
```


</br>

_Cross Validation with tune_grid(): LASSO_
```{r}
lasso_resample <- 
  lasso_wf %>%
  tune_grid(resamples = fold_BS,
            grid = LASSO_grid,
            control = control_grid(verbose = FALSE, save_pred = TRUE),
            metrics = metric_set(rmse))

lasso_resample %>%
  collect_metrics()
```

</br>

_Plot model performance using autoplot()_
```{r}
#Plot of MIDO data LASSO model performance
lasso_resample %>%
  autoplot(main="MIDO Data LASSO Performance")


```

_Showing and selecting best performing Models_
```{r}
#Showing best performing LASSO models
top_five_bs <- lasso_resample %>%
  show_best()
  
#Selects best performing model
best_lasso <- lasso_resample %>%
  select_best()

Best_LASSO_BS <- kable(top_five_bs,
      caption = "Top 5 best performing LASSO Models for Biological Score as the Outcome")

Best_LASSO_BS
```

_This shows that model 47 is the best performing models (RMSE = 6.50; STE = 0.16). However, it doesn't really perform any better than the null model, making it a bad fit to the data._

</br>

_Creating final fit based on best model permutation and plotting predicted values from that final fit model_
```{r}
lasso_final_wf <- 
  lasso_wf %>% 
  finalize_workflow(best_lasso)

lasso_final_wf

#Create workflow for fitting model to train_data2 predictions
lasso_final_fit <- 
  lasso_final_wf %>%
  fit(MIDO_LASSO) 
```

_Calculating residuals:_
```{r}
#Manually calculate residuals for MIDO data LASSO models between real and predicted values.
lasso_residuals <- lasso_final_fit %>%
  augment(MIDO_LASSO) %>% #use augment() to make predictions from train data
  select(c(.pred, biological_score)) %>%
  mutate(.resid = biological_score - .pred) #calculate residuals and make new row.

lasso_residuals
```
_model predictions from tuned model vs actual outcomes_
```{r}
lasso_pred_plot <- ggplot(lasso_residuals, 
                          aes(x = biological_score, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: LASSO", 
       x = "Biological Score Outcome", 
       y = "Biological Score Prediction")
lasso_pred_plot

#save plot
lasso_pred_plot_BS_location <- here::here("results", "lasso_prediction_plot_BS.png")

ggsave(lasso_pred_plot_BS_location)
```

</br>

_plot residuals vs predictions_
```{r}
lasso_residual_plot <- ggplot(lasso_residuals, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: LASSO", 
       x = "Biological Score Prediction", 
       y = "Residuals")
plot(lasso_residual_plot) #view plot
```

# LASSO: E. coli cfu

## Data Set Up:

_Set seed: This sets a random number generator with initial (pseudo)random values set as "123". We will need a series of random numbers created for our machine learning analysis._
```{r}
#Set random number generator.
set.seed(123)
```

</br>

## Null Model For MIDO Data

_5-fold cross validation, 5 times repeated for MIDO data: Here we are setting a cross-validation of the machine learning models. Cross-validation is used to measure how the results of our machine learning models will generalize to an independent data set. As such, the folds created will be be 5 random sub-samples of the train data set to test the validity of our models within the train data set. The 5x5 structure is arbitrary._
```{r}
#Create folds for LASSO Cross Validation.
fold_EC <- vfold_cv(MIDO_LASSO, v = 5, repeats = 5, strata = e.coli.cfu)
```

_Creating the recipe for E. coli cfu vs all predictors_

```{r}
EC.recipe <- 
  recipe(e.coli.cfu ~ ., data = MIDO_LASSO) %>%
  step_zv(all_predictors()) %>%
  step_normalize(all_predictors())
  
EC.recipe
```
_Setting linear regression model to assess relationship between E. coli cfu (outcome) and all other predictor variables._
```{r}
lm_mod <- linear_reg() %>% set_engine("lm") %>% set_mode("regression")
```

_However, first we need to create our null model to test against._

</br>

### Null Model:

 </br>

_Creates null model recipe. When we call this term, it will indicate in our workflow that E. Coli cfu will be predicted by a value of 1 (NULL)._ 
```{r}
Null_recipe_lm_EC <- recipe(e.coli.cfu ~ 1, data = MIDO_LASSO)
```

_Creating the Workflow: this creates a set workflow for running a null linear regression model with E. coli cfu as the outcome._
```{r}
null_wf_EC <- workflow() %>% add_model(lm_mod) %>% add_recipe(Null_recipe_lm_EC)
```

_Here, I am going to fit the null model created in the above workflow to the folds made from the train data set._
```{r}
null_lm_EC <- fit_resamples(null_wf_EC, resamples = fold_EC)
```

</br>

_Calculate RMSE for the train data linear model._
```{r}
Null_Met_EC <- collect_metrics(null_lm_EC)


Null_Met_EC <- kable(Null_Met_EC,
      caption = "Root Mean Squared Error and Standard Deviation for E. coli cfu Null Linear Model")

Null_Met_EC
```
_RMSE = 0.52, with a standard deviation of 0.014. This will serve as our check to test our models against latter on._

</br>

## LASSO Model

</br>

_Specifying The Model: LASSO_
```{r}
lasso_mod <- 
  linear_reg(penalty = tune(), mixture = 1) %>% 
  set_engine("glmnet")
```

</br>

_Creating a Workflow: LASSO_
```{r}
lasso_wf_EC <- workflow() %>%
  add_model(lasso_mod) %>%
  add_recipe(EC.recipe)
```

</br>

_Create Tuning Grid: LASSO_
```{r}
LASSO_grid <- grid_regular(penalty(), levels = 50)
```


</br>

_Cross Validation with tune_grid(): LASSO_
```{r}
lasso_resample_EC <- 
  lasso_wf_EC %>%
  tune_grid(resamples = fold_EC,
            grid = LASSO_grid,
            control = control_grid(verbose = FALSE, save_pred = TRUE),
            metrics = metric_set(rmse))

lasso_resample_EC %>%
  collect_metrics()
```

</br>

_Plot model performance using autoplot()_
```{r}
#Plot of MIDO data LASSO model performance
lasso_resample_EC %>%
  autoplot()
```

_Showing and selecting best performing Models_
```{r}
#Showing best performing LASSO models
top_five_ec <- lasso_resample_EC %>%
  show_best()
  
#Selects best performing model
best_lasso_EC <- lasso_resample_EC %>%
  select_best()

Best_LASSO_EC <- kable(top_five_ec,
      caption = "Top 5 best performing LASSO Models for E. coli cfu as the Outcome")

Best_LASSO_EC
```

_This shows that model 41 is the best performing models (RMSE = 0.50; STE = 0.012). However, it doesn't really perform any better than the null model, making it a bad fit to the data._

</br>

_Creating final fit based on best model permutation and plotting predicted values from that final fit model_
```{r}
lasso_final_wf_EC <- 
  lasso_wf_EC %>% 
  finalize_workflow(best_lasso_EC)

lasso_final_wf_EC

#Create workflow for fitting model to train_data2 predictions
lasso_final_fit_EC <- 
  lasso_final_wf_EC %>%
  fit(MIDO_LASSO) 
```

_Calculating residuals:_
```{r}
#Manually calculate residuals for MIDO data LASSO models between real and predicted values.
lasso_residuals_EC <- lasso_final_fit_EC %>%
  augment(MIDO_LASSO) %>% #use augment() to make predictions from train data
  select(c(.pred, e.coli.cfu)) %>%
  mutate(.resid = e.coli.cfu - .pred) #calculate residuals and make new row.

lasso_residuals_EC
```
_model predictions from tuned model vs actual outcomes_
```{r}
lasso_pred_plot_EC <- ggplot(lasso_residuals_EC, 
                          aes(x = e.coli.cfu, 
                              y = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Actual: LASSO", 
       x = "E. coli cfu Outcome", 
       y = "E. coli cfu Prediction")
lasso_pred_plot_EC

#save plot
lasso_pred_plot_EC_location <- here::here("results", "lasso_prediction_plot_EC.png")

ggsave(lasso_pred_plot_EC_location)
```
_plot residuals vs predictions_
```{r}
lasso_residual_plot_EC <- ggplot(lasso_residuals_EC, 
                              aes(y = .resid, 
                                  x = .pred)) + 
  geom_point() + 
  labs(title = "Predictions vs Residuals: LASSO", 
       x = "E. coli cfu Prediction", 
       y = "Residuals")
plot(lasso_residual_plot_EC) #view plot
```
